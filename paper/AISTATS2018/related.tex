\section{Related Work}
\label{related}
%Two works, learning the structure on the observed variables and discovering latent variables and the structure of the complete latent gaussian graphical model
%GGM l1\\
GGM provide an efficient representation of the concentration matrix through a graph that represents non-zeros in the matrix \citep{lauritzen1996graphical}. In high-dimensional regimes, this graph can be forced to be sparse, imposing a low-dimensional structure on the GGM. \citet{yuan2007model} and \citet{banerjee2008model} proposed the graphical lasso estimator to impose sparsity on the concentration matrix. \citet{tan2014learning} consider a new convex penalty function to encourage the presence of ‘hub’ nodes, nodes with many neighbors.\\
%GGM with other regularizations\\
%LGGM\\
In the context of LVGGM \citet{chandrasekaran2010} imposes a sparse plus low rank structure on concentration matrix marginalized over latent variables and proposes a convex formulation to learn the structure on the observed variables and identifying the number of latent variables but not their effects on observed variables. In order to speed
up the estimation of the sparse plus low-rank components,\citet{xu2017speeding} propose a sparsity constrained maximum likelihood estimator based on matrix factorization, and an efficient alternating gradient descent algorithm with hard thresholding to solve it. \citet{hosseini2016learning}  present a bi-convex formulation to jointly learn both a network among observed variables and densely connected and overlaping groups of variables, revealing the existence of potential latent variables. 
%\citet{ambroise2009inferring} use an Expectation-Maximization approach for variational estimation of the latent structure while inferring the network among the entire variable.
%\citet{xu2017speeding}  In order to speed
%up the estimation of the sparse plus low-rank components, we propose a sparsity constrained maximum likelihood estimator based on matrix factorization, and an efficient alternating gradient descent algorithm with hard thresholding to solve it.\\
%
%\citet{hosseini2016learning}  present a framework to jointly learn both a network among observed variables and densely connected groups and overlaping groups of variables. revealing the existence of potential latent variables. Not convex, alternate.

%
%ref in hosseini
%\citet{marlin2009sparse} MAP estimation with a prior that prefers sparse graphs. In this paper, we use the stochastic block model as a prior
%\citet{tan2015cluster} linkage on the variables
%
%\citet{celik2014efficient}
%
%Ambroise et al., 2009) uses an Expectation-
%Maximization approach for variational estimation of the latent structure while inferring the network among the entire variable