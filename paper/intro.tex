\section{Introduction}
\label{intro}


Graphical models have emerged as useful tools for modelling complex systems. In many fields such as genomics and finance among others, we have to analyze high-dimensional data, where the dimension of the sample $p$ is of the same order or larger than the number of samples $n$, yielding to ill-conditioned problems where some form of regularization needs to be imposed.

In the context of Gaussian Graphical Models (GGM) the central problem is often to estimate the inverse covariance matrix, also known as the precision or concentration matrix. The sparsity pattern of the concentration matrix in such models corresponds to the structure of the graph, i.e. the nonzeros of the concentration matrix correspond to the edges of the graphical model. The model selection method usually studied in such a GGM setting is the graphical Lasso $\ell_1$-regularized maximum-likelihood \citep{friedman2008sparse,yuan2007model,banerjee2008model}.

In some applications, some effects can affect the data, that is, some of the relevant variables may be latent. In applications where latent variables are missing, the marginalized precision matrix may not be sparse. The resulting marginal precision matrix of the Latent Variable Gaussian Graphical Model (LVGGM) has a sparse plus low-rank structure. \citet{chandrasekaran2010} consider a regularized Maximum Likelihood approach, using $\ell_1$-norm for the sparse component and trace norm for the low-rank component. With the obtained decomposition one identifies the structure of the conditional graphical model on the observed variables and the subspace of latent variables. However the decomposition fails to identify the effect of each latent variable separately since the subspace of latent variables does not have a unique decomposition. Choosing the orthogonal decomposition (SVD) gives us one choice. In our setup we want to impose some additional structure to the low-rank component in order to be able to identify the complete LVGGM structure.  

\citet{richard2014tight} introduce a new matrix norm that used as a regularizer yields to  estimates for low-rank matrices with multiple sparse factors that are provably more efficient statistically than the $\ell_1$ and trace norms.

In this work, we propose to impose more structure to the low rank matrix using a generalization of the trace norm introduced in Richard et al as a regularizer. This decomposition yields to a better interpretability of the graphical model structure recovering the structure of the complete graphical model. We propose a convex formulation with a quadratic loss function that can be optimized efficiently with the algorithm proposed in \citet{vinyes2017}.  Finally we study the identifiability of such decomposition. 

The paper is structured as follows. In Section \ref{related} we review the relevant prior literature. In Section 3 we formulate the LVGGM estimation problem as a regularized convex problem that imposes a sparsity structure on the latent variables, we also propose a tractable algorithm. In Section 4 we study identifiability conditions. Experimental results are shown in Section 5.

\subsection*{Notations}
$\itgset{p}$ denotes the set ${1,...,p}$ and $\mathcal{G}^p_k$ denotes the
set of subsets of $k$ elements in $\itgset{p}$. $|I|$ denotes the cardinality of a set $I$. If $v\in\RR^{p}$ is a vector, $\supp(v)$ denotes its support. If $M\in\RR^{p\times p}$ is a matrix, $I\subset\itgset{n}$ , $M_{II}\in\RR^{|I|\times |I|}$ is the submatrix obtained by selecting the rows and columns indexed by $I$ in $M$. For a symmetric matrix $M$, $\lambda_{\max}^+(M)$ is the largest positive eigenvalue and zero if they are all negative.

%In many applications as genetics, finance...learning structure of graphical models. with more interpretability and more efficient inference if sparse. (cf drton for application examples). Speciffically, if the number of variables is very large compared to the number of samples
%We focus on learning undirected gaussian graphical models. A well studied problem is model selection where the inverse of the covariance, also called concentration matrix, is sparse. The nonzeros of the concentration matrix correspond to the edges of the graphical model. Graphical Lasso is the method of choice.
%Here model selection problem with latent variables. In applications where latent variables are missing, this may create non sparsity for some components on the graph, fully connecting all the observed variables connected to the latent variable, see Figure. 
%Why learning GGM, then latent GGM to learn models with more interpretability and more efficient inference if sparse. With no a priori on the number of latent variables.
%An interesting idea is to add sparsity to latent components. Theory on decomposition, more difficult if sparse latent components. 
%
%figure latent graphical model