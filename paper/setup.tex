\section{Problem setup}
\label{setup}

\subsection{Latent Variable GGM}
\label{sec:ggm}
We consider the problem of Latent-Variable Gaussian Graphical Model (LVGGM) selection. Let $O$ and $H$ be the set of indexes of observed variables and latent variables respectively. Let $p$ be the number of observed variables, $|O|=p$, and  $h$ the number of latent variables $|H|=h$. We denote $\Sigma\in\RR^{(p+h)\times(p+h)}$ the complete covariance matrix and $K=\Sigma^{-1}$ the complete concentration matrix. $\hat{\Sigma}\in\RR^{(p+h)\times(p+h)}$ denotes the empirical covariance matrix. We only have access to the empirical marginal covariance matrix $\hat{\Sigma}_{OO}$. The Schur complement of $K$ with respect to block $K_{HH}$ gives 

\begin{align}
\Sigma_{OO}^{-1} = K_{OO}-K_{OH}K_{HH}^{-1}K_{HO}.
\end{align}

$K_{OO}$ specifies conditional dependency of observed variables given the latent variables and $K_{OH}$ specifies the effects of latent variables on observed variables. Assuming the model is sparse and has a small number of latent variables, $K_{OO}$ is a sparse matrix and $K_{OH}K_{HH}^{-1}K_{HO}$ is low rank. \citet{chandrasekaran2010} suggest to approximate $\hat{\Sigma}$  by $S-L$ where $S$ is sparse and $L$ is low rank, and propose a convex relaxation

\begin{align}
\label{opt_tr}
\min_{S,L} f(S-L)+\mu\|S\|_{1}+\lambda \tr(L) \quad s.t. \quad S-L \succeq 0 \quad L \succeq 0,
\end{align}

where the function $f$ is the loss function. Typically,in graphical model seletion $f$ is the log-likelihood

\begin{align}
f_{ML}(K)&:=\log\det(K) - \tr(K\hat{\Sigma})
\end{align}

However $\textit{logdet}$ is not Lipschitz preventing us from applying efficient algorithms of the family of conditional gradient, for which there is not convergence results for this setting.  Two other natural losses, which have the advantage of being quadratic, are the second order Taylor expansion arount the identity matrix of the log-likelihood $f_{T}$ and the score matching loss $f_{SM}$, introduced by \citet{hyvarinen2005estimation} and used for graphical model estimation in \citet{lin2016estimation},
\begin{align}
f_{T}(K)&:=\frac{1}{2}\|\hat{\Sigma}^{1/2}K\hat{\Sigma}^{1/2}-I\|_2^2\\
f_{SM}(K)&:=\frac{1}{2}\tr(K^2 \hat{\Sigma})-\tr(K).
\end{align}

\citet{chandrasekaran2010} show that the regularized maximum log-likelihood formulation \ref{opt_tr} provides estimates  that have the same sparsity pattern and rank than $K_{OO}$ and $K_{OH}K_{HH}^{-1}K_{HO}$. The low rank component of the estimate only retrieves latent variable subspace but not its structure.\\

If we assume that the latent variables are independent, i.e. $K_{HH}$ diagonal, the low rank component writes $UU^{\top}$, where $U\in\RR^{p\times h}$. Unicity of the decomposition $UU^{\top}$ can be achieved by more structure on the low rank component, which we discuss in the next paragraph.Under appropriate identifiability conditions , discussed in section \ref{sec:id}), it allows us to retrieve the structure of the full LVGGM. In order to obtain a convex formulation we introduce a norm for low rank \textit{positive semidefinite} (p.s.d.) matrices with multiple sparse factors. 

%and that the decomposition is unique (see section \ref{sec:id} on identifiability).  Unicity of $K_{OH}$ is achieved imposing more structure on the low rank component, which we discuss in the next paragraph.
 
%talk here about structure of $K_{OO}-K_{OH}K_{HH}^{-1}K_{HO}.$  structure $S-L$ explain\\

%and three losses ML and the two quadratic\\

%$UU$ decomposition with sparsity on columns... Transition, in order to obtain convex formulation..\\
%
%\begin{align}
%FIGURE ?
%\end{align}

\subsection{$k,\succeq$-rank and its relaxation}
\label{subsec:norm}

\citet{richard2014tight} propose a new matrix atomic norm that yield to estimates for low-rank matrices with multiple sparse factors. Based on the same ideas we define a norm \footnote{In fact it is a gauge. For more on gauges see \citet{chandrasekaran2010convex} and references therein.}  that yields to estimates with sparse and p.s.d factors. We assume that the sparsity of the factors is known and fixed and discuss a generalization for factors of different sparsity levels at the end of the section.

The following definition generalizes the notion of rank for p.s.d matrices.
\begin{mydef}
($k,\succeq$-rank) For a p.s.d  matrix $Z\in\RR^{p\times p}$ we define its $k,\succeq$-rank as the optimal
value of the optimization problem:
\begin{align}
\min \|c\|_0 \quad \text{s.t.} \quad Z=\sum_{i} c_i u_i u_{i}^\top, \quad c_i\in \RR^{+} \quad u_{i}\in\RR^p  :   \|u_{i}\|_0 \leq k, \|u_{i}\|_2 = 1.
\end{align}
\end{mydef}
Note that not all p.s.d matrices can have such a decomposition, so $k,\succeq$-rank can be infinite. We can derive a convex relaxation of the $k,\succeq$-rank, is the gauge $\Omega_{k,\succeq}$ \footnote{$\Omega_{k,\succeq}$ is not a norm but only a gauge because the set $\{u u^\top \in\RR^p  :   \|u\|_0 \leq k, \|u\|_2 = 1\}$  is not centrally symmetric} . 
\begin{mydef}
($\Omega_{k,\succeq}$) For a p.s.d  matrix $Z\in\RR^{p\times p}$ 
\begin{align}
\Omega_{k,\succeq}(Z):=\min \|c\|_1 \quad Z=\sum_{i} c_i u_i u_{i}^\top, \quad c_i\in \RR^{+} \quad u_{i}\in\RR^p  :   \|u_{i}\|_0 \leq k, \|u_{i}\|_2 = 1
\end{align}
Equivalently, as shown in \citet{richard2014tight}, 
\begin{align}
\Omega_{k,\succeq}(Z):=\inf_{Z_I, I\in\mathcal{G}^p_k} \sum_{I}\tr(Z_I) \quad \text{s.t.} \quad Z_I\succeq 0 ,\supp(Z_I)=I\times I
\end{align}
%where  $\mathcal{A}_{I,\succeq}:=\{Z_I\succeq 0 ,\supp(Z_I)=I\times I\}$.
\end{mydef}

We can have $\Omega_{k,\succeq}(Z)=+\infty$ even if $Z$ is p.s.d., if $Z$ cannot be decomposed in $k$-sparse,  rank-1 p.s.d. factors.

\begin{lemm} The polar gauge of $\Omega_{k,\succeq}$ of a symmetric $Y\RR^{p\times p}$ is
\begin{align}
\Omega_{k,\succeq}^{\circ}(Y):= \max_{I\in\mathcal{G}^p_k}\lambda^{+}_{max}(Y_{II})
\end{align}
\end{lemm}

$\Omega_{k,\succeq}$ can be generalised for different sparsity levels as
\begin{align}
\Omega_{\succeq}(Z):=\inf \sum_{k,i}w_{k}c_i^k \quad \text{s.t.} \quad Z=\sum_{k,i} c_i^k u_i^k u_{i}^{k\top}, \quad c_i^k\in \RR^{+} \quad u_{i}^k\in\RR^p  :   \|u_{i}^k\|_0 \leq k, \|u_{i}^k\|_2 = 1,
\end{align}
where $w_{k}$ is an increasing cardinality function that penalises each sparsity level $k$ by $w_{k}$. We illustrate this generalisation in the experiments.

\subsection{Convex formulation}
%Let $(x_1,..,x_n)$ be $n$ samples of dimension $p$ and   $\hat{\Sigma}$ the empirical covariance. A natural way to approximate a given sample covariance matrix by a model which the concentration matrix decomposes into a sparse and $k$-low-rank matrix is a regularized using $\ell_1$ norm for recovering sparse component and the convex relaxation of $k$-rank for recovering the $k$-low-rank component. Hence we consider the following convex optimization problem
%\begin{align}
%\label{opt}
%\min_{S,L} f(S-L)+\mu\|S\|_{1}+\lambda\Omega_k(L) \quad s.t. \quad S-L \succeq 0 \quad L \succeq 0,
%\end{align}
%where  $f$ represents the loss function.
% Typically,in Graphical model seletion $f$ is the log-likelihood, as proposed in \citet{chandrasekaran2010}. However \textit{logdet} is not Lipschitz preventing us from applying efficient algorithms of the family of conditional gradient, for which there is not convergence results for this setting.  Two other natural losses, which have the advantage of being quadratic, are the second order taylor expansion of the log-likelihood $f_{T}$ and the score matching loss $f_{SM}$, introduced by \citet{hyvarinen2005estimation} and used for graphical model estimation in \citet{lin2016estimation}.
%\begin{align}
%f_{SM}(K)&:=\frac{1}{2}\tr(K^2 \hat{\Sigma})-\tr(K) \\
%f_{T}(K)&:=\frac{1}{2}\|\hat{\Sigma}^{1/2}K\hat{\Sigma}^{1/2}-I\|_2^2.
%\end{align}

We use $\Omega_{k,\succeq}$ to impose structure to the low rank component and consider the following convex optimization problem,
\begin{align}
\label{opt}
\min_{S,L} f(S-L)+\mu\|S\|_{1}+\lambda\Omega_k(L) \quad s.t. \quad S-L \succeq 0 \quad L \succeq 0.
\end{align}
Since the norm $\Omega_k$ only provides symmetric p.s.d matrices, as a sum of p.s.d rank-one matrices, the nonnegativity constraint on $L$ can be dropped. The regularization $\mu\|S\|_{1}+\lambda\Omega_k(L)$ defines an atomic norm itself as shown in the following lemma

\begin{lemm} $\gamma(M):=\inf\{\mu\|A\|_{1}+\lambda\Omega_k(B)|M=A+B\}$ is an atomic norm
\end{lemm}
Thus our problem is rewritten as
\begin{align}
\label{opt_at}
\min_{K} f(K)+ \gamma(K) \quad s.t. \quad K \succeq 0,
\end{align}
and $K$ writes as a sum of atoms of $\ell_1$ and atoms of $\Omega_k$. Therefore we can recover a sparse component and a low-rank component with multiple sparse factors.\\

\subsection{Algorithm}
We choose to consider quadratic losses $f_{T}$ and $f_{T}$, introduced in section \ref{sec:ggm}, this allowing us to use an efficient algorithm for optimization. Optimization problem \ref{opt_at} is a minimization of a quadratic funtion regularized by an atomic norm.\TODO{What about $S-L \succeq 0$} We use algorithm for optimizing problems regularized by atomic norms from \citet{vinyes2017} LMO is the polar gauge,  we use truncated power iteration of \citet{yuan2013truncated}, after adding the frobenius norm of the matrix.\\

explain three loops, first we add support, then atoms on support, then as 

 \begin{algorithm}
   \caption{Column generation}
\label{alg:colgen}
\begin{algorithmic}[1]
\State\textbf{Require: } $f$ convex differentiable, tolerance $\epsilon$ 
\State\textbf{Initialization: } $x^0=0$, $A^0=\varnothing$, $k_0=0$, $t=1$
\Repeat
\State $a_{t}\gets \arg\max_{a \in \A} \dt{-\nabla f(x^{t-1})}{a}$ %\Comment New atom selection : using oracle for $\A$
\State $A^t \gets [A^{t-1},a_{t}]$
\State $c^t\gets \arg\min_{c \geq 0} f \big (A^{t} c \big ) + \|c\|_1$ %\Comment Relaxed primal : solved with active-set procedure
\State $I\gets \{i \mid c_i^t>0\}$,
%\State $k_t \gets |I|$ 
\State $c^t \gets c^t_{I}$
\State $A^t \gets A^t_{\cdot,I}$
\State $x^t\gets A^{t}c^t$
\State $t \gets t+1$
\Until $\max_{a \in \A} \dt{-\nabla f(x^{t-1})}{a}\leq \epsilon$
\end{algorithmic}
  \end{algorithm}





